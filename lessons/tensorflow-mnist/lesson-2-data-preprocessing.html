<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Poppins:100,200,300,400,500,600,700,800,900&amp;subset=devanagari,latin-ext" rel="stylesheet">
    <title>Lesson 2: Data Preprocessing & MNIST - TensorFlow Course</title>
    <link rel="shortcut icon" type="image/icon" href="../../assets/logo/favicon.png"/>
    <link rel="stylesheet" href="../../assets/css/font-awesome.min.css">
    <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../assets/css/bootsnav.css">	
    <link rel="stylesheet" href="../../assets/css/style.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
      .lesson-container { min-height: 100vh; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); padding: 30px 0; }
      .lesson-header { text-align: center; color: #333; margin-bottom: 40px; padding: 40px 0; background: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1); border-radius: 15px; }
      .lesson-header h1 { font-size: 2.8rem; margin-bottom: 15px; color: #ff6b35; }
      .lesson-header .lesson-meta { display: flex; justify-content: center; gap: 30px; flex-wrap: wrap; margin-top: 20px; font-size: 1rem; color: #666; }
      .back-link { position: absolute; top: 20px; left: 20px; color: #ff6b35; font-size: 1.2rem; text-decoration: none; transition: all 0.3s ease; z-index: 10; }
      .back-link:hover { color: #e55a2b; text-decoration: none; }
      .lesson-content { background: white; border-radius: 15px; padding: 40px; box-shadow: 0 5px 20px rgba(0,0,0,0.1); margin-bottom: 30px; }
      .lesson-content h2 { color: #ff6b35; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 3px solid #f0f2f5; }
      .lesson-content h3 { color: #333; margin: 25px 0 15px 0; }
      .lesson-content p { line-height: 1.8; color: #555; margin-bottom: 20px; }
      .code-section { background: linear-gradient(135deg, #ff6b35 0%, #f7931e 100%); color: white; padding: 30px; border-radius: 15px; margin: 30px 0; }
      .code-example { background: #2d3748; border-radius: 10px; padding: 20px; margin: 20px 0; overflow-x: auto; }
      .code-example pre { margin: 0; background: none !important; padding: 0 !important; }
      .code-example code { color: #e2e8f0; font-family: 'Courier New', monospace; font-size: 0.9rem; }
      .output-box { background: #1a202c; color: #e2e8f0; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #4299e1; }
      .exercise-box { background: #e8f4fd; border-left: 4px solid #007bff; padding: 25px; border-radius: 10px; margin: 30px 0; }
      .exercise-box h3 { color: #007bff; margin-bottom: 20px; }
      .dataset-info { background: #d4edda; border-left: 4px solid #28a745; padding: 25px; border-radius: 10px; margin: 30px 0; }
      .dataset-info h3 { color: #28a745; margin-bottom: 20px; }
      .navigation-buttons { display: flex; justify-content: space-between; margin-top: 40px; flex-wrap: wrap; gap: 15px; }
      .btn-nav { padding: 12px 25px; border-radius: 25px; text-decoration: none; font-weight: 600; transition: all 0.3s ease; border: none; cursor: pointer; }
      .btn-primary { background: linear-gradient(135deg, #ff6b35 0%, #f7931e 100%); color: white; }
      .btn-primary:hover { color: white; text-decoration: none; box-shadow: 0 5px 15px rgba(255, 107, 53, 0.4); }
      .btn-secondary { background: #6c757d; color: white; }
      .btn-secondary:hover { background: #5a6268; color: white; text-decoration: none; }
      .tip-box { background: #fff3cd; border-left: 4px solid #ffc107; padding: 20px; border-radius: 8px; margin: 20px 0; }
      .tip-box h4 { color: #856404; margin-bottom: 10px; }
      .image-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 15px; margin: 20px 0; }
      .mnist-sample { text-align: center; padding: 15px; background: #f8f9fa; border-radius: 8px; }
    </style>
  </head>
  
  <body>
    <a href="../tensorflow-course.html" class="back-link"><i class="fa fa-arrow-left"></i> Back to Course</a>
    
    <div class="lesson-container">
      <div class="container">
        <div class="lesson-header">
          <h1><i class="fa fa-database"></i> Data Preprocessing & MNIST</h1>
          <p>Load, explore, and prepare the famous MNIST dataset for neural network training</p>
          <div class="lesson-meta">
            <span><i class="fa fa-clock-o"></i> 30-40 minutes</span>
            <span><i class="fa fa-bar-chart"></i> Beginner Level</span>
            <span><i class="fa fa-code"></i> Hands-on Coding</span>
          </div>
        </div>

        <div class="lesson-content">
          <h2>The MNIST Dataset</h2>
          <p>MNIST (Modified National Institute of Standards and Technology) is the "Hello World" of computer vision. It's a dataset of 70,000 handwritten digits (0-9) that's perfect for learning neural networks because it's:</p>
          
          <ul>
            <li><strong>Simple:</strong> Small 28x28 pixel grayscale images</li>
            <li><strong>Clean:</strong> Pre-processed and labeled</li>
            <li><strong>Manageable:</strong> Fast to train on most computers</li>
            <li><strong>Educational:</strong> Easy to visualize and understand results</li>
          </ul>

          <div class="dataset-info">
            <h3><i class="fa fa-info-circle"></i> MNIST Dataset Details</h3>
            <ul>
              <li><strong>Training Images:</strong> 60,000 samples</li>
              <li><strong>Test Images:</strong> 10,000 samples</li>
              <li><strong>Image Size:</strong> 28x28 pixels (784 features)</li>
              <li><strong>Classes:</strong> 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</li>
              <li><strong>Pixel Values:</strong> 0-255 (grayscale intensity)</li>
              <li><strong>File Size:</strong> ~12MB total</li>
            </ul>
          </div>

          <h2>Loading the MNIST Dataset</h2>
          <p>TensorFlow makes it incredibly easy to load MNIST. It's built right into Keras datasets!</p>
          
          <div class="code-section">
            <h3>ðŸš€ Loading MNIST with TensorFlow</h3>
            <div class="code-example">
              <pre><code class="language-python">import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Load the MNIST dataset
print("Loading MNIST dataset...")
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Print dataset information
print(f"Training data shape: {x_train.shape}")
print(f"Training labels shape: {y_train.shape}")
print(f"Test data shape: {x_test.shape}")
print(f"Test labels shape: {y_test.shape}")

# Print data types and value ranges
print(f"Image data type: {x_train.dtype}")
print(f"Label data type: {y_train.dtype}")
print(f"Pixel value range: {x_train.min()} to {x_train.max()}")
print(f"Unique labels: {np.unique(y_train)}")</code></pre>
            </div>
            
            <div class="output-box">
              <strong>Expected Output:</strong><br>
              Loading MNIST dataset...<br>
              Training data shape: (60000, 28, 28)<br>
              Training labels shape: (60000,)<br>
              Test data shape: (10000, 28, 28)<br>
              Test labels shape: (10000,)<br>
              Image data type: uint8<br>
              Label data type: uint8<br>
              Pixel value range: 0 to 255<br>
              Unique labels: [0 1 2 3 4 5 6 7 8 9]
            </div>
          </div>

          <h2>Exploring the Data</h2>
          <p>Before training any model, it's crucial to understand your data. Let's visualize some MNIST images and analyze their distribution.</p>

          <h3>Visualizing Sample Images</h3>
          <div class="code-example">
            <pre><code class="language-python"># Function to display MNIST images
def plot_mnist_samples(images, labels, num_samples=10):
    """Display a grid of MNIST images with their labels"""
    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    axes = axes.ravel()
    
    for i in range(num_samples):
        axes[i].imshow(images[i], cmap='gray')
        axes[i].set_title(f'Label: {labels[i]}')
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.show()

# Display first 10 training images
print("Sample training images:")
plot_mnist_samples(x_train, y_train, 10)</code></pre>
          </div>

          <h3>Analyzing Label Distribution</h3>
          <div class="code-example">
            <pre><code class="language-python"># Analyze the distribution of labels
import collections

# Count occurrences of each digit
train_counts = collections.Counter(y_train)
test_counts = collections.Counter(y_test)

print("Training set label distribution:")
for digit in range(10):
    print(f"Digit {digit}: {train_counts[digit]} samples")

print("\nTest set label distribution:")
for digit in range(10):
    print(f"Digit {digit}: {test_counts[digit]} samples")

# Visualize distribution
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.bar(train_counts.keys(), train_counts.values())
plt.title('Training Set Distribution')
plt.xlabel('Digit')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
plt.bar(test_counts.keys(), test_counts.values())
plt.title('Test Set Distribution')
plt.xlabel('Digit')
plt.ylabel('Count')

plt.tight_layout()
plt.show()</code></pre>
          </div>

          <h2>Data Preprocessing</h2>
          <p>Raw data is rarely ready for machine learning. We need to preprocess it to improve training performance and accuracy.</p>

          <h3>1. Normalization</h3>
          <p>Neural networks work best with normalized data. We'll scale pixel values from [0, 255] to [0, 1].</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Normalize pixel values to range [0, 1]
print("Before normalization:")
print(f"Min value: {x_train.min()}, Max value: {x_train.max()}")

# Convert to float32 and normalize
x_train_normalized = x_train.astype('float32') / 255.0
x_test_normalized = x_test.astype('float32') / 255.0

print("After normalization:")
print(f"Min value: {x_train_normalized.min()}, Max value: {x_train_normalized.max()}")
print(f"Data type: {x_train_normalized.dtype}")

# Compare original vs normalized image
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

ax1.imshow(x_train[0], cmap='gray')
ax1.set_title(f'Original (0-255)\nLabel: {y_train[0]}')
ax1.axis('off')

ax2.imshow(x_train_normalized[0], cmap='gray')
ax2.set_title(f'Normalized (0-1)\nLabel: {y_train[0]}')
ax2.axis('off')

plt.tight_layout()
plt.show()</code></pre>
          </div>

          <div class="tip-box">
            <h4><i class="fa fa-lightbulb-o"></i> Why Normalize?</h4>
            <p>Normalization helps because:</p>
            <ul>
              <li><strong>Faster Training:</strong> Smaller numbers mean faster computations</li>
              <li><strong>Better Convergence:</strong> Optimization algorithms work better with normalized data</li>
              <li><strong>Prevents Overflow:</strong> Large numbers can cause numerical instability</li>
              <li><strong>Fair Feature Comparison:</strong> All pixels contribute equally to learning</li>
            </ul>
          </div>

          <h3>2. Reshaping Data</h3>
          <p>Different neural network architectures expect different input shapes. Let's prepare our data for both dense and convolutional networks.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Option 1: Flatten for Dense/Fully Connected networks
x_train_flat = x_train_normalized.reshape(x_train_normalized.shape[0], -1)
x_test_flat = x_test_normalized.reshape(x_test_normalized.shape[0], -1)

print("Flattened shapes:")
print(f"Training: {x_train_flat.shape}")
print(f"Test: {x_test_flat.shape}")

# Option 2: Add channel dimension for Convolutional networks
x_train_conv = x_train_normalized.reshape(x_train_normalized.shape[0], 28, 28, 1)
x_test_conv = x_test_normalized.reshape(x_test_normalized.shape[0], 28, 28, 1)

print("\nConvolutional shapes:")
print(f"Training: {x_train_conv.shape}")
print(f"Test: {x_test_conv.shape}")

# Visualize the difference
print("\nShape comparison:")
print(f"Original: {x_train_normalized.shape} -> Flattened: {x_train_flat.shape}")
print(f"784 = 28 x 28 (width x height)")
print(f"Channel dimension: {x_train_conv.shape} (samples, height, width, channels)")</code></pre>
          </div>

          <h3>3. One-Hot Encoding Labels</h3>
          <p>For multi-class classification, we convert integer labels to one-hot encoded vectors.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Convert labels to one-hot encoding
num_classes = 10

y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes)

print("Label encoding comparison:")
print(f"Original labels: {y_train[:5]}")
print(f"One-hot encoded:")
for i in range(5):
    print(f"  {y_train[i]} -> {y_train_onehot[i]}")

print(f"\nShapes:")
print(f"Original: {y_train.shape}")
print(f"One-hot: {y_train_onehot.shape}")

# Visualize one-hot encoding
plt.figure(figsize=(10, 6))
sample_idx = 0
digit_label = y_train[sample_idx]

plt.subplot(1, 2, 1)
plt.imshow(x_train[sample_idx], cmap='gray')
plt.title(f'Digit: {digit_label}')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.bar(range(10), y_train_onehot[sample_idx])
plt.title('One-Hot Encoding')
plt.xlabel('Digit Class')
plt.ylabel('Value')
plt.xticks(range(10))

plt.tight_layout()
plt.show()</code></pre>
          </div>

          <h2>Creating Data Pipelines</h2>
          <p>TensorFlow's tf.data API provides efficient data loading and preprocessing pipelines.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Create TensorFlow datasets
batch_size = 32

# Create training dataset
train_dataset = tf.data.Dataset.from_tensor_slices((x_train_normalized, y_train))
train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Create test dataset
test_dataset = tf.data.Dataset.from_tensor_slices((x_test_normalized, y_test))
test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

print(f"Training batches: {len(train_dataset)}")
print(f"Test batches: {len(test_dataset)}")

# Inspect a batch
for batch_x, batch_y in train_dataset.take(1):
    print(f"Batch shape: {batch_x.shape}")
    print(f"Labels shape: {batch_y.shape}")
    print(f"First few labels: {batch_y[:5].numpy()}")
    break</code></pre>
          </div>

          <h2>Data Augmentation (Optional)</h2>
          <p>Data augmentation creates variations of existing data to improve model generalization.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Simple data augmentation for MNIST
def augment_data(image, label):
    """Apply random transformations to images"""
    # Random rotation (Â±10 degrees)
    image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))
    
    # Random brightness adjustment
    image = tf.image.random_brightness(image, 0.1)
    
    # Ensure values stay in [0, 1] range
    image = tf.clip_by_value(image, 0.0, 1.0)
    
    return image, label

# Apply augmentation to training data
train_dataset_aug = train_dataset.map(augment_data)

# Visualize augmented samples
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
original_batch = next(iter(train_dataset))
augmented_batch = next(iter(train_dataset_aug))

for i in range(5):
    # Original
    axes[0, i].imshow(original_batch[0][i], cmap='gray')
    axes[0, i].set_title(f'Original: {original_batch[1][i].numpy()}')
    axes[0, i].axis('off')
    
    # Augmented
    axes[1, i].imshow(augmented_batch[0][i], cmap='gray')
    axes[1, i].set_title(f'Augmented: {augmented_batch[1][i].numpy()}')
    axes[1, i].axis('off')

plt.tight_layout()
plt.show()</code></pre>
          </div>

          <div class="exercise-box">
            <h3><i class="fa fa-code"></i> Hands-On Exercise</h3>
            <p>Now it's your turn! Complete this data preprocessing pipeline:</p>
            
            <h4>Exercise: Complete MNIST Preprocessing</h4>
            <ol>
              <li>Load the MNIST dataset</li>
              <li>Create a function to display the distribution of each digit</li>
              <li>Normalize the data and create both flattened and convolutional versions</li>
              <li>Split the training data into train/validation sets (80/20)</li>
              <li>Create TensorFlow datasets with appropriate batching</li>
            </ol>
            
            <div class="code-example">
              <pre><code class="language-python"># Your solution here
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# 1. Load MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 2. Distribution function
def plot_digit_distribution(labels, title="Digit Distribution"):
    """Plot the distribution of digits in the dataset"""
    # Your code here
    pass

# 3. Normalize and reshape
def preprocess_data(x_train, x_test, y_train, y_test):
    """Normalize and reshape the data"""
    # Your code here
    return x_train_processed, x_test_processed, y_train_processed, y_test_processed

# 4. Train/validation split
def create_train_val_split(x_train, y_train, val_split=0.2):
    """Split training data into train and validation sets"""
    # Your code here
    return x_train_split, x_val_split, y_train_split, y_val_split

# 5. Create TensorFlow datasets
def create_datasets(x_train, y_train, x_val, y_val, x_test, y_test, batch_size=32):
    """Create TensorFlow datasets"""
    # Your code here
    return train_ds, val_ds, test_ds

# Test your functions
plot_digit_distribution(y_train, "Training Set Distribution")
# ... rest of your implementation</code></pre>
            </div>
          </div>

          <h2>Key Takeaways</h2>
          <p>In this lesson, you've learned the essential data preprocessing steps:</p>
          <ul>
            <li><strong>Dataset Loading:</strong> Using TensorFlow's built-in datasets</li>
            <li><strong>Data Exploration:</strong> Understanding your data through visualization and statistics</li>
            <li><strong>Normalization:</strong> Scaling data for better neural network performance</li>
            <li><strong>Data Reshaping:</strong> Preparing data for different network architectures</li>
            <li><strong>Label Encoding:</strong> Converting labels for multi-class classification</li>
            <li><strong>Data Pipelines:</strong> Creating efficient data loading with tf.data</li>
            <li><strong>Data Augmentation:</strong> Expanding your dataset artificially</li>
          </ul>
          
          <p>Next, we'll use this preprocessed MNIST data to build our first neural network from scratch!</p>
        </div>

        <div class="navigation-buttons">
          <a href="lesson-1-tensorflow-basics.html" class="btn-nav btn-secondary">
            <i class="fa fa-arrow-left"></i> Previous: TensorFlow Basics
          </a>
          <a href="lesson-3-first-neural-network.html" class="btn-nav btn-primary">
            Next: First Neural Network <i class="fa fa-arrow-right"></i>
          </a>
        </div>
      </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <script>
      // Mark lesson as completed when user scrolls to bottom
      window.addEventListener('scroll', function() {
        if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight - 100) {
          localStorage.setItem('tensorflow_lesson_2_completed', 'true');
        }
      });
    </script>
  </body>
</html>
