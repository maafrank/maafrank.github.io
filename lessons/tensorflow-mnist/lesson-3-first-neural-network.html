<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Poppins:100,200,300,400,500,600,700,800,900&amp;subset=devanagari,latin-ext" rel="stylesheet">
    <title>Lesson 3: Building Your First Neural Network - TensorFlow Course</title>
    <link rel="shortcut icon" type="image/icon" href="../../assets/logo/favicon.png"/>
    <link rel="stylesheet" href="../../assets/css/font-awesome.min.css">
    <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../assets/css/bootsnav.css">	
    <link rel="stylesheet" href="../../assets/css/style.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
      .lesson-container { min-height: 100vh; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); padding: 30px 0; }
      .lesson-header { text-align: center; color: #333; margin-bottom: 40px; padding: 40px 0; background: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1); border-radius: 15px; }
      .lesson-header h1 { font-size: 2.8rem; margin-bottom: 15px; color: #ff6b35; }
      .lesson-header .lesson-meta { display: flex; justify-content: center; gap: 30px; flex-wrap: wrap; margin-top: 20px; font-size: 1rem; color: #666; }
      .back-link { position: absolute; top: 20px; left: 20px; color: #ff6b35; font-size: 1.2rem; text-decoration: none; transition: all 0.3s ease; z-index: 10; }
      .back-link:hover { color: #e55a2b; text-decoration: none; }
      .lesson-content { background: white; border-radius: 15px; padding: 40px; box-shadow: 0 5px 20px rgba(0,0,0,0.1); margin-bottom: 30px; }
      .lesson-content h2 { color: #ff6b35; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 3px solid #f0f2f5; }
      .lesson-content h3 { color: #333; margin: 25px 0 15px 0; }
      .lesson-content p { line-height: 1.8; color: #555; margin-bottom: 20px; }
      .code-section { background: linear-gradient(135deg, #ff6b35 0%, #f7931e 100%); color: white; padding: 30px; border-radius: 15px; margin: 30px 0; }
      .code-example { background: #2d3748; border-radius: 10px; padding: 20px; margin: 20px 0; overflow-x: auto; }
      .code-example pre { margin: 0; background: none !important; padding: 0 !important; }
      .code-example code { color: #e2e8f0; font-family: 'Courier New', monospace; font-size: 0.9rem; }
      .output-box { background: #1a202c; color: #e2e8f0; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #4299e1; }
      .exercise-box { background: #e8f4fd; border-left: 4px solid #007bff; padding: 25px; border-radius: 10px; margin: 30px 0; }
      .exercise-box h3 { color: #007bff; margin-bottom: 20px; }
      .architecture-box { background: #d4edda; border-left: 4px solid #28a745; padding: 25px; border-radius: 10px; margin: 30px 0; }
      .architecture-box h3 { color: #28a745; margin-bottom: 20px; }
      .navigation-buttons { display: flex; justify-content: space-between; margin-top: 40px; flex-wrap: wrap; gap: 15px; }
      .btn-nav { padding: 12px 25px; border-radius: 25px; text-decoration: none; font-weight: 600; transition: all 0.3s ease; border: none; cursor: pointer; }
      .btn-primary { background: linear-gradient(135deg, #ff6b35 0%, #f7931e 100%); color: white; }
      .btn-primary:hover { color: white; text-decoration: none; box-shadow: 0 5px 15px rgba(255, 107, 53, 0.4); }
      .btn-secondary { background: #6c757d; color: white; }
      .btn-secondary:hover { background: #5a6268; color: white; text-decoration: none; }
      .tip-box { background: #fff3cd; border-left: 4px solid #ffc107; padding: 20px; border-radius: 8px; margin: 20px 0; }
      .tip-box h4 { color: #856404; margin-bottom: 10px; }
      .layer-diagram { background: #f8f9fa; border: 2px solid #dee2e6; padding: 20px; border-radius: 10px; margin: 20px 0; text-align: center; }
    </style>
  </head>
  
  <body>
    <a href="../tensorflow-course.html" class="back-link"><i class="fa fa-arrow-left"></i> Back to Course</a>
    
    <div class="lesson-container">
      <div class="container">
        <div class="lesson-header">
          <h1><i class="fa fa-building"></i> Building Your First Neural Network</h1>
          <p>Create a feedforward neural network to classify MNIST digits</p>
          <div class="lesson-meta">
            <span><i class="fa fa-clock-o"></i> 45-60 minutes</span>
            <span><i class="fa fa-bar-chart"></i> Beginner Level</span>
            <span><i class="fa fa-code"></i> Hands-on Coding</span>
          </div>
        </div>

        <div class="lesson-content">
          <h2>Neural Network Architecture</h2>
          <p>A neural network is like a complex function that learns to map inputs to outputs. For MNIST digit classification, we need to map 28Ã—28 pixel images to one of 10 digit classes (0-9).</p>
          
          <div class="architecture-box">
            <h3><i class="fa fa-sitemap"></i> Our First Network Architecture</h3>
            <div class="layer-diagram">
              <p><strong>Input Layer:</strong> 784 neurons (28Ã—28 flattened pixels)</p>
              <p>â†“</p>
              <p><strong>Hidden Layer 1:</strong> 128 neurons + ReLU activation</p>
              <p>â†“</p>
              <p><strong>Hidden Layer 2:</strong> 64 neurons + ReLU activation</p>
              <p>â†“</p>
              <p><strong>Output Layer:</strong> 10 neurons + Softmax activation (one per digit)</p>
            </div>
            <p><strong>Total Parameters:</strong> ~101,000 trainable parameters!</p>
          </div>

          <h2>Setting Up Our Environment</h2>
          <p>Let's start by importing libraries and loading our preprocessed MNIST data.</p>
          
          <div class="code-section">
            <h3>ðŸš€ Environment Setup</h3>
            <div class="code-example">
              <pre><code class="language-python">import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from tensorflow.keras.utils import plot_model

# Set random seeds for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

print(f"TensorFlow version: {tf.__version__}")
print(f"GPU available: {tf.config.list_physical_devices('GPU')}")

# Load and preprocess MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten images for our dense network
x_train_flat = x_train.reshape(-1, 28 * 28)
x_test_flat = x_test.reshape(-1, 28 * 28)

print(f"Training data shape: {x_train_flat.shape}")
print(f"Test data shape: {x_test_flat.shape}")
print(f"Number of classes: {len(np.unique(y_train))}")</code></pre>
            </div>
            
            <div class="output-box">
              <strong>Expected Output:</strong><br>
              TensorFlow version: 2.13.0<br>
              GPU available: []<br>
              Training data shape: (60000, 784)<br>
              Test data shape: (10000, 784)<br>
              Number of classes: 10
            </div>
          </div>

          <h2>Building the Model with Keras Sequential API</h2>
          <p>Keras provides multiple ways to build models. The Sequential API is perfect for stacking layers one after another.</p>

          <h3>Method 1: Sequential Model</h3>
          <div class="code-example">
            <pre><code class="language-python"># Create a Sequential model
model = tf.keras.Sequential([
    # Input layer (implicitly defined by first layer)
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    
    # Hidden layer
    tf.keras.layers.Dense(64, activation='relu'),
    
    # Output layer
    tf.keras.layers.Dense(10, activation='softmax')
])

# Display model architecture
print("Model Architecture:")
model.summary()

# Visualize the model (optional)
tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)</code></pre>
          </div>

          <h3>Method 2: Sequential with add() Method</h3>
          <div class="code-example">
            <pre><code class="language-python"># Alternative way to build the same model
model_alt = tf.keras.Sequential()

# Add layers one by one
model_alt.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)))
model_alt.add(tf.keras.layers.Dense(64, activation='relu'))
model_alt.add(tf.keras.layers.Dense(10, activation='softmax'))

print("Alternative model summary:")
model_alt.summary()</code></pre>
          </div>

          <div class="tip-box">
            <h4><i class="fa fa-lightbulb-o"></i> Understanding the Architecture</h4>
            <ul>
              <li><strong>Input Shape (784,):</strong> Flattened 28Ã—28 = 784 pixel values</li>
              <li><strong>Dense Layers:</strong> Fully connected layers where each neuron connects to all neurons in the previous layer</li>
              <li><strong>ReLU Activation:</strong> Rectified Linear Unit - outputs max(0, x), adds non-linearity</li>
              <li><strong>Softmax Activation:</strong> Converts outputs to probabilities that sum to 1</li>
              <li><strong>128, 64 neurons:</strong> Common sizes that balance capacity and efficiency</li>
            </ul>
          </div>

          <h2>Understanding Layers and Activations</h2>
          
          <h3>Dense (Fully Connected) Layers</h3>
          <div class="code-example">
            <pre><code class="language-python"># Let's examine a single Dense layer
single_layer = tf.keras.layers.Dense(64, activation='relu', input_shape=(784,))

# Create a mini-model to see the layer in action
test_model = tf.keras.Sequential([single_layer])
test_model.build(input_shape=(None, 784))

print(f"Layer weights shape: {single_layer.weights[0].shape}")  # (784, 64)
print(f"Layer bias shape: {single_layer.weights[1].shape}")     # (64,)
print(f"Total parameters in this layer: {784 * 64 + 64}")

# Test with a single image
test_input = x_train_flat[0:1]  # Shape: (1, 784)
test_output = test_model(test_input)
print(f"Input shape: {test_input.shape}")
print(f"Output shape: {test_output.shape}")
print(f"Output sample: {test_output[0][:10].numpy()}")  # First 10 values</code></pre>
          </div>

          <h3>Activation Functions in Detail</h3>
          <div class="code-example">
            <pre><code class="language-python"># Compare different activation functions
x = np.linspace(-5, 5, 100)

# ReLU: max(0, x)
relu = np.maximum(0, x)

# Sigmoid: 1 / (1 + e^(-x))
sigmoid = 1 / (1 + np.exp(-x))

# Tanh: (e^x - e^(-x)) / (e^x + e^(-x))
tanh = np.tanh(x)

# Plot comparison
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(x, relu, 'b-', linewidth=2)
plt.title('ReLU Activation')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(x, sigmoid, 'r-', linewidth=2)
plt.title('Sigmoid Activation')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(x, tanh, 'g-', linewidth=2)
plt.title('Tanh Activation')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)

plt.tight_layout()
plt.show()

print("Activation function properties:")
print("ReLU: Fast, helps with vanishing gradients, but can 'die'")
print("Sigmoid: Smooth, but suffers from vanishing gradients")
print("Tanh: Zero-centered, better than sigmoid for hidden layers")</code></pre>
          </div>

          <h2>Model Compilation</h2>
          <p>Before training, we need to compile the model by specifying the optimizer, loss function, and metrics.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Compile the model
model.compile(
    optimizer='adam',           # Adaptive learning rate optimizer
    loss='sparse_categorical_crossentropy',  # For integer labels
    metrics=['accuracy']        # Track accuracy during training
)

print("Model compiled successfully!")
print("\nCompilation choices explained:")
print("Optimizer: Adam - adaptive learning rate, works well for most problems")
print("Loss: Sparse categorical crossentropy - for multi-class classification with integer labels")
print("Metrics: Accuracy - percentage of correct predictions")

# Alternative compilation with different settings
model_alt.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
)

print("\nAlternative compilation with explicit objects:")</code></pre>
          </div>

          <div class="tip-box">
            <h4><i class="fa fa-lightbulb-o"></i> Loss Function Choices</h4>
            <ul>
              <li><strong>sparse_categorical_crossentropy:</strong> Use when labels are integers (0, 1, 2, ...)</li>
              <li><strong>categorical_crossentropy:</strong> Use when labels are one-hot encoded</li>
              <li><strong>binary_crossentropy:</strong> Use for binary classification (0 or 1)</li>
            </ul>
          </div>

          <h2>Model Inspection and Visualization</h2>
          <p>Let's examine our model more closely before training.</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Detailed model information
print("=== MODEL SUMMARY ===")
model.summary()

print(f"\n=== TRAINABLE PARAMETERS ===")
total_params = model.count_params()
print(f"Total parameters: {total_params:,}")

print(f"\n=== LAYER DETAILS ===")
for i, layer in enumerate(model.layers):
    print(f"Layer {i}: {layer.name}")
    print(f"  Type: {type(layer).__name__}")
    print(f"  Output shape: {layer.output_shape}")
    print(f"  Parameters: {layer.count_params():,}")
    if hasattr(layer, 'activation'):
        print(f"  Activation: {layer.activation.__name__}")
    print()

# Test forward pass with a single example
print("=== FORWARD PASS TEST ===")
test_image = x_train_flat[0:1]  # Single image
predictions = model(test_image)
print(f"Input shape: {test_image.shape}")
print(f"Output shape: {predictions.shape}")
print(f"Raw predictions: {predictions[0].numpy()}")
print(f"Predicted class: {np.argmax(predictions[0])}")
print(f"Actual class: {y_train[0]}")
print(f"Confidence: {np.max(predictions[0]):.4f}")</code></pre>
          </div>

          <h2>Understanding Model Predictions (Before Training)</h2>
          <p>Let's see what our untrained model predicts - this will be essentially random!</p>
          
          <div class="code-example">
            <pre><code class="language-python"># Test untrained model on several examples
num_samples = 5
test_indices = np.random.choice(len(x_train), num_samples, replace=False)

print("=== UNTRAINED MODEL PREDICTIONS ===")
plt.figure(figsize=(15, 6))

for i, idx in enumerate(test_indices):
    # Get prediction
    image = x_train_flat[idx:idx+1]
    prediction = model(image)
    predicted_class = np.argmax(prediction[0])
    confidence = np.max(prediction[0])
    actual_class = y_train[idx]
    
    # Plot image
    plt.subplot(2, num_samples, i+1)
    plt.imshow(x_train[idx], cmap='gray')
    plt.title(f'Actual: {actual_class}')
    plt.axis('off')
    
    # Plot prediction probabilities
    plt.subplot(2, num_samples, i+1+num_samples)
    plt.bar(range(10), prediction[0])
    plt.title(f'Pred: {predicted_class} ({confidence:.2f})')
    plt.xlabel('Digit')
    plt.ylabel('Probability')
    plt.xticks(range(10))

plt.tight_layout()
plt.show()

print("Notice how the untrained model's predictions are essentially random!")
print("The probabilities are roughly equal across all classes.")</code></pre>
          </div>

          <div class="exercise-box">
            <h3><i class="fa fa-code"></i> Hands-On Exercise</h3>
            <p>Now it's your turn to build and experiment with neural networks!</p>
            
            <h4>Exercise 1: Build a Different Architecture</h4>
            <p>Create a neural network with the following specifications:</p>
            <ul>
              <li>3 hidden layers with 256, 128, and 64 neurons</li>
              <li>Use 'relu' activation for hidden layers</li>
              <li>Use 'softmax' for the output layer</li>
              <li>Compile with Adam optimizer and sparse categorical crossentropy</li>
            </ul>
            
            <div class="code-example">
              <pre><code class="language-python"># Your solution here
def create_custom_model():
    """Create a neural network with 3 hidden layers"""
    model = tf.keras.Sequential([
        # Your code here
        # Add layers according to specifications
    ])
    
    # Compile the model
    # Your code here
    
    return model

# Test your model
custom_model = create_custom_model()
custom_model.summary()

# Compare parameter counts
print(f"Original model parameters: {model.count_params():,}")
print(f"Your model parameters: {custom_model.count_params():,}")</code></pre>
            </div>
            
            <h4>Exercise 2: Experiment with Activations</h4>
            <p>Create models with different activation functions and compare their architectures:</p>
            
            <div class="code-example">
              <pre><code class="language-python"># Create models with different activations
def create_model_with_activation(activation):
    """Create a model with specified activation function"""
    # Your code here
    pass

# Test different activations
activations = ['relu', 'tanh', 'sigmoid']
models_dict = {}

for activation in activations:
    models_dict[activation] = create_model_with_activation(activation)
    print(f"\n{activation.upper()} Model:")
    models_dict[activation].summary()
    
    # Test prediction on same image
    test_pred = models_dict[activation](x_train_flat[0:1])
    print(f"Sample prediction range: [{test_pred[0].numpy().min():.4f}, {test_pred[0].numpy().max():.4f}]")</code></pre>
            </div>
          </div>

          <h2>Model Architecture Best Practices</h2>
          
          <div class="architecture-box">
            <h3><i class="fa fa-lightbulb-o"></i> Design Guidelines</h3>
            <ul>
              <li><strong>Start Simple:</strong> Begin with 1-2 hidden layers, add complexity gradually</li>
              <li><strong>Layer Sizes:</strong> Common pattern is decreasing size (784 â†’ 128 â†’ 64 â†’ 10)</li>
              <li><strong>Activation Functions:</strong> ReLU for hidden layers, softmax for multi-class output</li>
              <li><strong>Parameter Count:</strong> More parameters = more capacity, but also more risk of overfitting</li>
              <li><strong>Depth vs Width:</strong> Deep networks (more layers) often work better than wide ones</li>
            </ul>
          </div>

          <h2>What's Next?</h2>
          <p>Congratulations! You've built your first neural network. Here's what you've accomplished:</p>
          <ul>
            <li><strong>Architecture Design:</strong> Created a multi-layer feedforward network</li>
            <li><strong>Layer Understanding:</strong> Learned about Dense layers and their parameters</li>
            <li><strong>Activation Functions:</strong> Explored ReLU, sigmoid, and softmax</li>
            <li><strong>Model Compilation:</strong> Set up optimizer, loss function, and metrics</li>
            <li><strong>Model Inspection:</strong> Analyzed the model structure and parameters</li>
          </ul>
          
          <p>Your model is ready but untrained - it's making random predictions! In the next lesson, we'll train this network and watch it learn to recognize digits with high accuracy.</p>
        </div>

        <div class="navigation-buttons">
          <a href="lesson-2-data-preprocessing.html" class="btn-nav btn-secondary">
            <i class="fa fa-arrow-left"></i> Previous: Data Preprocessing
          </a>
          <a href="lesson-4-training-optimization.html" class="btn-nav btn-primary">
            Next: Training & Optimization <i class="fa fa-arrow-right"></i>
          </a>
        </div>
      </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <script>
      // Mark lesson as completed when user scrolls to bottom
      window.addEventListener('scroll', function() {
        if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight - 100) {
          localStorage.setItem('tensorflow_lesson_3_completed', 'true');
        }
      });
    </script>
  </body>
</html>